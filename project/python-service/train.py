import pandas as pd
import torch
import lightning as L
from torch.utils.data import DataLoader
from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet
from pytorch_forecasting.data import GroupNormalizer
from pytorch_forecasting.metrics import MAE

# --------------------- 0. è¶…å‚æ•° ---------------------
# Sample
# CSV_PATH = r"E:\Yoobee\MSE907\Github\data\processed\retail\sample_retail_item_store.csv"

# Full
CSV_PATH = r"E:\Yoobee\MSE907\Github\data\processed\retail\retail_item_store.csv"
ENCODER_LENGTH = 5  # å†å²çª—å£
DECODER_LENGTH = 2  # é¢„æµ‹çª—å£
BATCH_SIZE = 64
EPOCHS = 10
LR = 1e-3

print("ğŸš€ å¯åŠ¨è®­ç»ƒè„šæœ¬â€¦")
print(f"CSV è·¯å¾„: {CSV_PATH}")

# --------------------- 1. è¯»æ•°æ® ---------------------
df = pd.read_csv(
    CSV_PATH,
    parse_dates=["date"],
    dtype={"store_id": "str"},  # é¿å… dtype è­¦å‘Š
    low_memory=False,  # 7 GB æ–‡ä»¶ä¸€æ¬¡è¯»å®Œ
)
df["store_id"] = df["store_id"].astype(str)

# 1.1 æŒ‰æ—¥èšåˆ
df_agg = df.groupby(["store_id", "date"], as_index=False)["unit_sales"].sum()

# 1.2 è¡¥å…¨æ—¥æœŸ
all_dates = pd.date_range(df_agg.date.min(), df_agg.date.max(), freq="D")
idx = pd.MultiIndex.from_product(
    [df_agg.store_id.unique(), all_dates], names=["store_id", "date"]
)
df_full = df_agg.set_index(["store_id", "date"]).reindex(idx).fillna(0).reset_index()
df_full["time_idx"] = (df_full["date"] - df_full["date"].min()).dt.days

# 1.3 å–æ¯åº—æœ€å ENCODER+DECODER å¤© & è¿‡æ»¤å…¨ 0
WINDOW = ENCODER_LENGTH + DECODER_LENGTH
frames = []
for sid, g in df_full.groupby("store_id"):
    g = g.sort_values("date")
    # æ‰¾æœ€é•¿è¿ç»­æ®µ
    block = (g.time_idx.diff() != 1).cumsum()
    longest = block.value_counts().idxmax()
    g = g[block == longest]
    if len(g) >= WINDOW and g.unit_sales.sum() > 0:
        frames.append(g.iloc[-WINDOW:].copy())

df_final = pd.concat(frames, ignore_index=True)
print(f"âœ… æœ€ç»ˆæ ·æœ¬ shape: {df_final.shape}, é—¨åº—æ•°: {df_final.store_id.nunique()}")

# --------------------- 2. å»º TimeSeriesDataSet ---------------------
training = TimeSeriesDataSet(
    df_final,
    time_idx="time_idx",
    target="unit_sales",
    group_ids=["store_id"],
    min_encoder_length=ENCODER_LENGTH,
    max_encoder_length=ENCODER_LENGTH,
    min_prediction_length=DECODER_LENGTH,
    max_prediction_length=DECODER_LENGTH,
    static_categoricals=["store_id"],
    time_varying_known_reals=["time_idx"],
    time_varying_unknown_reals=["unit_sales"],
    target_normalizer=GroupNormalizer(groups=["store_id"]),
    add_relative_time_idx=True,
    add_target_scales=True,
    add_encoder_length=True,
    randomize_length=False,  # å…³é”® 1
    allow_missing_timesteps=False,  # å…³é”® 2
)

# 2.1 æ£€æµ‹ â€œåçª—å£â€ (è¿”å› None çš„æ ·æœ¬)
bad_idx = [i for i in range(len(training)) if training[i] is None]
if bad_idx:
    print(f"âš ï¸ å‘ç° {len(bad_idx)} ä¸ªçª—å£è¢«ä¸¢å¼ƒ (ç´¢å¼•ç¤ºä¾‹: {bad_idx[:10]})")
else:
    print("ğŸ‘ æ•°æ®é›†ä¸­æ²¡æœ‰åçª—å£")

# --------------------- 3. DataLoader ---------------------
train_loader: DataLoader = training.to_dataloader(
    train=True,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=0,
)

# quick sanity check
x, y = next(iter(train_loader))
print("ğŸŸ¢ ç¬¬ä¸€ä¸ª batch åŠ è½½æˆåŠŸ")
print("   â€£ x keys:", list(x.keys()))

target, weight = y  # y æ˜¯ (target, weight)
print("   â€£ target shape:", target.shape)
print("   â€£ weight:", "None" if weight is None else weight.shape)

# --------------------- 4. å®šä¹‰ TFT ---------------------
tft = TemporalFusionTransformer.from_dataset(
    training,
    learning_rate=LR,
    hidden_size=32,
    attention_head_size=4,
    dropout=0.1,
    hidden_continuous_size=16,
    output_size=1,  # å…³é”® 3
    loss=MAE(),
    log_interval=10,
    reduce_on_plateau_patience=4,
)

# --------------------- 5. Trainer ---------------------
trainer = L.Trainer(
    max_epochs=EPOCHS,
    accelerator="gpu" if torch.cuda.is_available() else "cpu",
    devices=1,
    gradient_clip_val=0.1,
    enable_checkpointing=False,  # demo é‡Œä¸ä¿å­˜ ckpt
    log_every_n_steps=1,
)


def count_trainable(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


print(f"ğŸ”§ å¯è®­ç»ƒå‚æ•°é‡: {count_trainable(tft):,}")

# --------------------- 6. å¼€å§‹è®­ç»ƒ ---------------------
trainer.fit(tft, train_dataloaders=train_loader)

# --------------------- 7. ä¿å­˜æ¨¡å‹ ---------------------
MODEL_PATH = "tft_big_model.pt"
torch.save(tft.state_dict(), MODEL_PATH)
print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜ä¸º {MODEL_PATH}")
