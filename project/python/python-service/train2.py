import pandas as pd
import torch
import lightning as L
from torch.utils.data import DataLoader
from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet
from pytorch_forecasting.data import GroupNormalizer
from pytorch_forecasting.metrics import MAE

# --------------------- 0. æ”¹è¿›çš„è¶…å‚æ•° ---------------------
CSV_PATH = r"E:\Yoobee\MSE907\Github\data\processed\retail\retail_item_store.csv"
ENCODER_LENGTH = 14  # å¢åŠ åˆ°2å‘¨å†å²
DECODER_LENGTH = 7  # é¢„æµ‹1å‘¨
BATCH_SIZE = 64
EPOCHS = 20  # å¢åŠ è®­ç»ƒè½®æ•°
LR = 1e-3

print("ğŸš€ å¯åŠ¨æ”¹è¿›çš„è®­ç»ƒè„šæœ¬â€¦")

# --------------------- 1. æ”¹è¿›çš„æ•°æ®å¤„ç† ---------------------
df = pd.read_csv(
    CSV_PATH,
    parse_dates=["date"],
    dtype={"store_id": "str"},
    low_memory=False,
)

# 1.1 æŒ‰æ—¥èšåˆ
df_agg = df.groupby(["store_id", "date"], as_index=False)["unit_sales"].sum()

# 1.2 è¡¥å…¨æ—¥æœŸ
all_dates = pd.date_range(df_agg.date.min(), df_agg.date.max(), freq="D")
idx = pd.MultiIndex.from_product(
    [df_agg.store_id.unique(), all_dates], names=["store_id", "date"]
)
df_full = df_agg.set_index(["store_id", "date"]).reindex(idx).fillna(0).reset_index()
df_full["time_idx"] = (df_full["date"] - df_full["date"].min()).dt.days

# 1.3 æ”¹è¿›çš„æ•°æ®è¿‡æ»¤ç­–ç•¥
MIN_DAYS = ENCODER_LENGTH + DECODER_LENGTH + 10  # è‡³å°‘éœ€è¦31å¤©æ•°æ®
MIN_SALES = 1  # è‡³å°‘æœ‰1æ¬¡é”€å”®

frames = []
for sid, g in df_full.groupby("store_id"):
    g = g.sort_values("date")

    # å¦‚æœé—¨åº—æœ‰è¶³å¤Ÿå¤©æ•°ä¸”æœ‰é”€å”®è®°å½•
    if len(g) >= MIN_DAYS and g.unit_sales.sum() > MIN_SALES:
        # ä¸åªå–æœ€åå‡ å¤©ï¼Œè€Œæ˜¯å–æ‰€æœ‰å¯ç”¨æ•°æ®
        frames.append(g.copy())

if frames:
    df_final = pd.concat(frames, ignore_index=True)
else:
    print("âŒ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„é—¨åº—æ•°æ®ï¼")
    exit()

print(f"âœ… æ•°æ®ç»Ÿè®¡:")
print(f"   â€£ æœ€ç»ˆæ ·æœ¬ shape: {df_final.shape}")
print(f"   â€£ é—¨åº—æ•°: {df_final.store_id.nunique()}")
print(f"   â€£ æ—¶é—´è·¨åº¦: {df_final.time_idx.max() - df_final.time_idx.min() + 1} å¤©")
print(f"   â€£ æ€»é”€å”®é¢: {df_final.unit_sales.sum():,.0f}")

# --------------------- 2. å»º TimeSeriesDataSet ---------------------
training = TimeSeriesDataSet(
    df_final,
    time_idx="time_idx",
    target="unit_sales",
    group_ids=["store_id"],
    min_encoder_length=ENCODER_LENGTH,
    max_encoder_length=ENCODER_LENGTH,
    min_prediction_length=DECODER_LENGTH,
    max_prediction_length=DECODER_LENGTH,
    static_categoricals=["store_id"],
    time_varying_known_reals=["time_idx"],
    time_varying_unknown_reals=["unit_sales"],
    target_normalizer=GroupNormalizer(groups=["store_id"]),
    add_relative_time_idx=True,
    add_target_scales=True,
    add_encoder_length=True,
    randomize_length=True,  # å…è®¸éšæœºé•¿åº¦å¢åŠ æ ·æœ¬å¤šæ ·æ€§
    allow_missing_timesteps=True,  # å…è®¸ç¼ºå¤±æ—¶é—´æ­¥
)

print(f"ğŸ“ˆ è®­ç»ƒé›†å¤§å°: {len(training):,} æ ·æœ¬")

# æ£€æŸ¥åæ ·æœ¬
bad_count = sum(1 for i in range(min(1000, len(training))) if training[i] is None)
print(f"âš ï¸ å‰1000ä¸ªæ ·æœ¬ä¸­åæ ·æœ¬æ•°: {bad_count}")

# --------------------- 3. DataLoader ---------------------
train_loader = training.to_dataloader(
    train=True,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=0,
)

print(f"ğŸ‹ï¸ è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")

# --------------------- 4. æ›´å¤§çš„ TFT æ¨¡å‹ ---------------------
tft = TemporalFusionTransformer.from_dataset(
    training,
    learning_rate=LR,
    hidden_size=128,  # å¢åŠ åˆ°128
    attention_head_size=8,  # å¢åŠ æ³¨æ„åŠ›å¤´
    dropout=0.2,  # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
    hidden_continuous_size=64,  # å¢åŠ è¿ç»­ç‰¹å¾éšè—å±‚
    output_size=1,
    loss=MAE(),
    log_interval=10,
    reduce_on_plateau_patience=5,
)

# å‚æ•°ç»Ÿè®¡
total_params = sum(p.numel() for p in tft.parameters())
trainable_params = sum(p.numel() for p in tft.parameters() if p.requires_grad)
print(f"ğŸ”§ æ¨¡å‹å‚æ•°:")
print(f"   â€£ æ€»å‚æ•°é‡: {total_params:,}")
print(f"   â€£ å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
print(f"   â€£ é¢„ä¼°æ¨¡å‹å¤§å°: {trainable_params * 4 / 1024 / 1024:.1f} MB (float32)")

# --------------------- 5. è®­ç»ƒ ---------------------
trainer = L.Trainer(
    max_epochs=EPOCHS,
    accelerator="gpu" if torch.cuda.is_available() else "cpu",
    devices=1,
    gradient_clip_val=0.1,
    enable_checkpointing=True,  # å¯ç”¨æ£€æŸ¥ç‚¹
    log_every_n_steps=10,
    default_root_dir="./tft_checkpoints",  # æ£€æŸ¥ç‚¹ç›®å½•
)

print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
trainer.fit(tft, train_dataloaders=train_loader)

# --------------------- 6. ä¿å­˜å®Œæ•´æ¨¡å‹ ---------------------
# æ–¹æ³•1: ä¿å­˜å®Œæ•´æ¨¡å‹
torch.save(
    {
        "model_state_dict": tft.state_dict(),
        "model_config": tft.hparams,
        "training_dataset_params": training.get_parameters(),
    },
    "tft_complete_model.pt",
)

# æ–¹æ³•2: ä½¿ç”¨ Lightning çš„ä¿å­˜æ–¹å¼
trainer.save_checkpoint("tft_lightning_model.ckpt")

# æ£€æŸ¥æ–‡ä»¶å¤§å°
import os

for filename in ["tft_complete_model.pt", "tft_lightning_model.ckpt"]:
    if os.path.exists(filename):
        size = os.path.getsize(filename)
        print(f"ğŸ’¾ {filename}: {size:,} bytes ({size/1024/1024:.1f} MB)")

print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
